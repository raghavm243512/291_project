{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246834\n",
      "244884\n",
      "242908\n",
      "239523\n",
      "220633\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlparse\n",
    "\n",
    "# Load the dataframe\n",
    "data = pd.read_parquet('cleaned.parquet')\n",
    "\n",
    "def analyze(item):\n",
    "    instr, out = item['instruction'], item['output']\n",
    "    statements = sqlparse.parse(instr)\n",
    "    tokens = []\n",
    "    for s in statements: # Multiple statements, combine into one big flattened list\n",
    "        tokens += list(s.flatten())\n",
    "\n",
    "    statements = sqlparse.parse(out)\n",
    "    for s in statements: # Multiple statements, combine into one big flattened list\n",
    "        tokens += list(s.flatten())\n",
    "    identifiers = [token.value for token in tokens if token.ttype.__repr__() in ['Token.Name', 'Token.Literal.String.Symbol', 'Token.Literal.String.Single']]\n",
    "    return max([len(i) for i in identifiers])\n",
    "\n",
    "mask = data.apply(analyze, axis=1)\n",
    "d1 = data[mask <= 36]\n",
    "d2 = data[mask <= 34]\n",
    "d3 = data[mask <= 32]\n",
    "d4 = data[mask <= 30]\n",
    "d5 = data[mask <= 28]\n",
    "\n",
    "# Print the length of the resulting dataframe\n",
    "print(len(d1))\n",
    "print(len(d2))\n",
    "print(len(d3))\n",
    "print(len(d4))\n",
    "print(len(d5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a new parquet file\n",
    "d6.to_parquet('new.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214561\n"
     ]
    }
   ],
   "source": [
    "d6 = data[mask <= 26]\n",
    "print(len(d6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlparse\n",
    "\n",
    "# Load the dataframe\n",
    "data = pd.read_parquet('data/he.jap.hi.parquet')\n",
    "#statements = sqlparse.parse(out)\n",
    "#for s in statements: # Multiple statements, combine into one big flattened list\n",
    "#    tokens += list(s.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chars(sql):\n",
    "    in_string = False\n",
    "    result = ''\n",
    "    end = len(sql) - 1\n",
    "    sizes = [0, end]\n",
    "    for i in range(end+1):\n",
    "        if sql[i] == \"'\":\n",
    "            in_string = not in_string\n",
    "        if sql[i] == '\"' and not in_string:\n",
    "            if i > 0 and sql[i-1] in ' .()':\n",
    "                result = f'{result}{sql[i]}'\n",
    "            elif i < end and sql[i+1] in ' .(),;':\n",
    "                result = f'{result}{sql[i]}'\n",
    "            elif i in sizes:\n",
    "                result = f'{result}{sql[i]}'\n",
    "        else:\n",
    "            result = f'{result}{sql[i]}'\n",
    "    \n",
    "    return result\n",
    "\n",
    "import sqlglot\n",
    "\n",
    "def valid_ast(sql):\n",
    "    try:\n",
    "        sqlglot.parse(sql, dialect=sqlglot.Dialects.SQLITE)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"पुरुषों\\ के\\ एकल\" FROM \"सारणी\\ \\(_43\\)\" WHERE \"लिंक\\ डबल\" = 'केनहीचक\\-चक\\-चिहरि'\n",
      "CREATE TABLE \"सारणी (_43)\" (\n",
      "    \"साल\" real,\n",
      "    \"पुरुषों के एकल\" text,\n",
      "    \"वंस के एकल\" text,\n",
      "    \"पुरुषों के डबल\" text,\n",
      "    \"ववन्स डबल\" text,\n",
      "    \"लिंक डबल\" text\n",
      ")\n",
      "\n",
      "\n",
      "--ऊपर दिए गए बक्सों के लिए नीचे दिए गए सवालों के जवाब दीजिए ।--कौन से पुरुषों की अविवाहितता है Tanen Haga-LY-चिया के साथ डबलs?\n"
     ]
    }
   ],
   "source": [
    "item = data.loc[200000]\n",
    "instr, out = item['instruction'], item['output']\n",
    "statements = sqlparse.parse(out)\n",
    "tokens = []\n",
    "for s in statements: # Multiple statements, combine into one big flattened list\n",
    "    tokens += list(s.flatten())\n",
    "print((''.join([token.value for token in tokens])))\n",
    "print(clean_chars(instr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236290\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_eng = pd.read_parquet('data/eng.parquet')\n",
    "new_data_eng = data_eng[~((data_eng['instruction'].str.contains('\\\\\\\\')) | (data_eng['output'].str.contains('\\\\\\\\')))]\n",
    "print(len(new_data_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT SUM(year) FROM table_name_93 WHERE edition > 4 AND winner = \"new york red bulls\"\n",
      "CREATE TABLE table_name_93 (\n",
      "    year INTEGER,\n",
      "    edition VARCHAR,\n",
      "    winner VARCHAR\n",
      ")\n",
      "\n",
      "\n",
      "-- Using valid SQLite, answer the following questions for the tables provided above.\n",
      "\n",
      "-- I want the sum of year for edition more than 4 and winners of new york red bulls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item = data_eng.loc[120000]\n",
    "instr, out = item['instruction'], item['output']\n",
    "print(out)\n",
    "print(instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DDL 'CREATE' at 0x26603D32620>, <Whitespace ' ' at 0x26603DD4880>, <Keyword 'TABLE' at 0x26603DD48E0>, <Whitespace ' ' at 0x26603DD4940>, <Name 'Manufa...' at 0x26603DD49A0>, <Whitespace ' ' at 0x26603DD4A00>, <Punctuation '(' at 0x26603DD4A60>, <Newline ' ' at 0x26603DD4AC0>, <Whitespace ' ' at 0x26603DD4B20>, <Whitespace ' ' at 0x26603DD4B80>, <Whitespace ' ' at 0x26603DD4BE0>, <Whitespace ' ' at 0x26603DD4C40>, <Name 'Code' at 0x26603DD4CA0>, <Whitespace ' ' at 0x26603DD4D00>, <Builtin 'INTEGER' at 0x26603DD4D60>, <Punctuation ',' at 0x26603DD4DC0>, <Newline ' ' at 0x26603DD4E20>, <Whitespace ' ' at 0x26603DD4E80>, <Whitespace ' ' at 0x26603DD4EE0>, <Whitespace ' ' at 0x26603DD4F40>, <Whitespace ' ' at 0x26603DD4FA0>, <Name 'Name' at 0x26603DD5000>, <Whitespace ' ' at 0x26603DD5060>, <Name 'VARCHAR' at 0x26603DD50C0>, <Punctuation '(' at 0x26603DD5120>, <Integer '255' at 0x26603DD5180>, <Punctuation ')' at 0x26603DD51E0>, <Punctuation ',' at 0x26603DD5240>, <Newline ' ' at 0x26603DD52A0>, <Whitespace ' ' at 0x26603DD5300>, <Whitespace ' ' at 0x26603DD5360>, <Whitespace ' ' at 0x26603DD53C0>, <Whitespace ' ' at 0x26603DD5420>, <Name 'Headqu...' at 0x26603DD5480>, <Whitespace ' ' at 0x26603DD54E0>, <Name 'VARCHAR' at 0x26603DD5540>, <Punctuation '(' at 0x26603DD55A0>, <Integer '255' at 0x26603DD5600>, <Punctuation ')' at 0x26603DD5660>, <Punctuation ',' at 0x26603DD56C0>, <Newline ' ' at 0x26603DD5720>, <Whitespace ' ' at 0x26603DD5780>, <Whitespace ' ' at 0x26603DD57E0>, <Whitespace ' ' at 0x26603DD5840>, <Whitespace ' ' at 0x26603DD58A0>, <Name 'Founder' at 0x26603DD5900>, <Whitespace ' ' at 0x26603DD5960>, <Name 'VARCHAR' at 0x26603DD59C0>, <Punctuation '(' at 0x26603DD5A20>, <Integer '255' at 0x26603DD5A80>, <Punctuation ')' at 0x26603DD5AE0>, <Punctuation ',' at 0x26603DD5B40>, <Newline ' ' at 0x26603DD5BA0>, <Whitespace ' ' at 0x26603DD5C00>, <Whitespace ' ' at 0x26603DD5C60>, <Whitespace ' ' at 0x26603DD5CC0>, <Whitespace ' ' at 0x26603DD5D20>, <Name 'Revenue' at 0x26603DD5D80>, <Whitespace ' ' at 0x26603DD5DE0>, <Builtin 'REAL' at 0x26603DD5E40>, <Newline ' ' at 0x26603DD5EA0>, <Punctuation ')' at 0x26603DD5F00>, <Newline ' ' at 0x26603DD5F60>, <Newline ' ' at 0x26603DD5FC0>, <DDL 'CREATE' at 0x26603DD6020>, <Whitespace ' ' at 0x26603DD6080>, <Keyword 'TABLE' at 0x26603DD60E0>, <Whitespace ' ' at 0x26603DD6140>, <Name 'Produc...' at 0x26603DD61A0>, <Whitespace ' ' at 0x26603DD6200>, <Punctuation '(' at 0x26603DD6260>, <Newline ' ' at 0x26603DD62C0>, <Whitespace ' ' at 0x26603DD6320>, <Whitespace ' ' at 0x26603DD6380>, <Whitespace ' ' at 0x26603DD63E0>, <Whitespace ' ' at 0x26603DD6440>, <Name 'Code' at 0x26603DD64A0>, <Whitespace ' ' at 0x26603DD6500>, <Builtin 'INTEGER' at 0x26603DD6560>, <Punctuation ',' at 0x26603DD65C0>, <Newline ' ' at 0x26603DD6620>, <Whitespace ' ' at 0x26603DD6680>, <Whitespace ' ' at 0x26603DD66E0>, <Whitespace ' ' at 0x26603DD6740>, <Whitespace ' ' at 0x26603DD67A0>, <Name 'Name' at 0x26603DD6800>, <Whitespace ' ' at 0x26603DD6860>, <Name 'VARCHAR' at 0x26603DD68C0>, <Punctuation '(' at 0x26603DD6920>, <Integer '255' at 0x26603DD6980>, <Punctuation ')' at 0x26603DD69E0>, <Punctuation ',' at 0x26603DD6A40>, <Newline ' ' at 0x26603DD6AA0>, <Whitespace ' ' at 0x26603DD6B00>, <Whitespace ' ' at 0x26603DD6B60>, <Whitespace ' ' at 0x26603DD6BC0>, <Whitespace ' ' at 0x26603DD6C20>, <Name 'Price' at 0x26603DD6C80>, <Whitespace ' ' at 0x26603DD6CE0>, <Builtin 'DECIMAL' at 0x26603DD6D40>, <Punctuation ',' at 0x26603DD6DA0>, <Newline ' ' at 0x26603DD6E00>, <Whitespace ' ' at 0x26603DD6E60>, <Whitespace ' ' at 0x26603DD6EC0>, <Whitespace ' ' at 0x26603DD6F20>, <Whitespace ' ' at 0x26603DD6F80>, <Name 'Manufa...' at 0x26603DD6FE0>, <Whitespace ' ' at 0x26603DD7040>, <Builtin 'INTEGER' at 0x26603DD70A0>, <Newline ' ' at 0x26603DD7100>, <Punctuation ')' at 0x26603DD7160>, <Newline ' ' at 0x26603DD71C0>, <Newline ' ' at 0x26603DD7220>, <Newline ' ' at 0x26603DD7280>, <Single '-- Usi...' at 0x26603DD72E0>, <Newline ' ' at 0x26603DD7340>, <Single '-- For...' at 0x26603DD73A0>]\n"
     ]
    }
   ],
   "source": [
    "statements = sqlparse.parse(instr)\n",
    "tokens.clear()\n",
    "for s in statements: # Multiple statements, combine into one big flattened list\n",
    "    tokens += list(s.flatten())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "#c1, c2, c3 = 0, 0, 0\n",
    "def clean(x):\n",
    "    return f'-- Utilizando SQLite válido, responda las siguientes preguntas para las tablas proporcionadas anteriormente.\\n-- {x[2]}'.replace('\\\\', '')\n",
    "\n",
    "data_eng = pd.read_parquet('data/test/eng_test.parquet')\n",
    "data = pd.read_parquet('data/test/es_test.parquet')\n",
    "\n",
    "# Split the 'instruction' column on '--'\n",
    "data_eng['instruction_split'] = data_eng['instruction'].str.split('--')\n",
    "data['instruction_split'] = data['instruction'].str.split('--')\n",
    "\n",
    "# Replace the first split of 'data' with the first split from 'data_eng'\n",
    "data['instruction_split'] = data_eng['instruction_split'].apply(lambda x: x[0]) + data['instruction_split'].apply(clean)\n",
    "\n",
    "# Replace the 'instruction' column in 'data' with the new string\n",
    "data['instruction'] = data['instruction_split']\n",
    "data.drop(columns=['instruction_split'], inplace=True)\n",
    "\n",
    "# Replace the 'output' column in 'data' with the one from 'data_eng'\n",
    "data['output'] = data_eng['output']\n",
    "\n",
    "data.to_parquet('es_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"player\" FROM table_203_815 WHERE \"college\" = 'kansas state'\n",
      "CREATE TABLE table_203_815 (\n",
      "    id number,\n",
      "    \"pick #\" number,\n",
      "    \"nfl team\" text,\n",
      "    \"player\" text,\n",
      "    \"position\" text,\n",
      "    \"college\" text\n",
      ")\n",
      "\n",
      "\n",
      "-- Utilizando SQLite válido, responda las siguientes preguntas para las tablas proporcionadas anteriormente.\n",
      "-- ¿Quién era el único jugador del estado de Kansas?\n"
     ]
    }
   ],
   "source": [
    "item = data.loc[0]\n",
    "instr, out = item['instruction'], item['output']\n",
    "print(out)\n",
    "print(instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            instruction  \\\n",
      "0     CREATE TABLE \"tabla_203_815\" (\\n    \"id\" numbe...   \n",
      "1     CREATE TABLE \"tabla_17232\" (\\n    \"Disco\" real...   \n",
      "2     CREATE TABLE \"curso\" (\\n    \"crs_code\" text,\\n...   \n",
      "3     CREATE TABLE \"tabla_4053\" (\\n    \"Persona\\ eje...   \n",
      "4     CREATE TABLE \"tabla_72445\" (\\n    \"Condado\" te...   \n",
      "...                                                 ...   \n",
      "2441  CREATE TABLE \"tabla_57826\" (\\n    \"Rank\" real,...   \n",
      "2442  CREATE TABLE \"tabla_24485\" (\\n    \"Día\" text,\\...   \n",
      "2443  CREATE TABLE \"Demografía\" (\\n    \"subject_id\" ...   \n",
      "2444  CREATE TABLE \"tabla_18598175_2\" (\\n    \"método...   \n",
      "2445  CREATE TABLE \"procedimientos\" (\\n    \"subject_...   \n",
      "\n",
      "                                                 output  \n",
      "0     SELECT \"jugador\" FROM \"tabla_203_815\" WHERE \"C...  \n",
      "1              SELECT \"MIN\"(\"Track\") FROM \"tabla_17232\"  \n",
      "2     SELECT \"T2\".\"emp_fname\", \"T4\".\"prof_office\", \"...  \n",
      "3     SELECT \"Bajo\\ el\\ Presidente\" FROM \"tabla_4053...  \n",
      "4     SELECT \"Ingreso\\ familiar\\ medio\" FROM \"tabla_...  \n",
      "...                                                 ...  \n",
      "2441  SELECT MAX(\"Bronze\") FROM \"tabla_57826\" WHERE ...  \n",
      "2442  SELECT \"Tiempo\" FROM \"tabla_24485\" WHERE \"Esta...  \n",
      "2443  SELECT \"CUENTA\"(DISTINCT \"Demografía\".\"subject...  \n",
      "2444  SELECT \"método_de_eliminación\" FROM \"tabla_185...  \n",
      "2445  SELECT \"CUENTA\"(DISTINCT \"Demografía\".\"subject...  \n",
      "\n",
      "[2446 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet('data/test/es_test.parquet')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating train split: 3000 examples [00:00, 356032.82 examples/s]\n",
      "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 3000/3000 [00:03<00:00, 791.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-large')\n",
    "\n",
    "def tokenize_data(examples):\n",
    "    inputs = tokenizer(examples['instruction'], truncation=False)\n",
    "    targets = tokenizer(examples['output'], truncation=False)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "path = 'data/test/eng_test.parquet'\n",
    "\n",
    "dataset = load_dataset('parquet', data_files=path)\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3000 examples [00:00, 239624.31 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:03<00:00, 769.81 examples/s]\n",
      "Generating train split: 3000 examples [00:00, 260407.95 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:03<00:00, 784.26 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/test/hi_qonly_test.parquet'\n",
    "dataset = load_dataset('parquet', data_files=path)\n",
    "tokenized_dataset_hi = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "path = 'data/test/he.jap.hi_qonly_test.parquet'\n",
    "dataset = load_dataset('parquet', data_files=path)\n",
    "tokenized_dataset_m = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "violators = set()\n",
    "\n",
    "for dataset in [tokenized_dataset, tokenized_dataset_hi, tokenized_dataset_m]:\n",
    "    for i in range(len(dataset['train'])):\n",
    "        if len(dataset['train'][i]['input_ids']) > 512 or len(dataset['train'][i]['labels']) > 512:\n",
    "            violators.add(i)\n",
    "\n",
    "len(violators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['data/test/eng_test.parquet', 'data/test/hi_qonly_test.parquet', 'data/test/he.jap.hi_qonly_test.parquet']\n",
    "import pandas as pd\n",
    "\n",
    "for path in paths:\n",
    "    df = pd.read_parquet(path)\n",
    "    df.drop(violators, inplace=True)\n",
    "    df.to_parquet(path.split('/')[-1], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63793, 63790, 63997)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "c1, c2, c3 = 0, 0, 0\n",
    "import unicodedata\n",
    "def count(x):\n",
    "    global c1, c2, c3\n",
    "    script = unicodedata.name(x.split('--')[1][1]).split(' ')[0]\n",
    "    if script in ['HIRAGANA', 'KATAKANA', 'CJK']:\n",
    "        c1 += 1\n",
    "        return x\n",
    "    elif script == 'HEBREW':\n",
    "        c2 += 1\n",
    "        return x\n",
    "    else:\n",
    "        c3 += 1\n",
    "        return x\n",
    "\n",
    "pd.read_parquet('data/train/he.jap.hi_qonly.parquet')['instruction'].apply(count)\n",
    "(c1, c2, c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gt = pd.read_parquet('data/test/eng_test.parquet')\n",
    "output = pd.read_parquet('model_outputs/base_eng_outputs.parquet')\n",
    "df = pd.concat([gt, output], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT T1.firstm_lname  T1.ef_office T3.prons_codescription, professor: T1 CLASSIN professor AS T2 ON T1.classf_num = T2.emp_numJOIN department AS T3 ON T1.classrs_code = T3.crs_codeJOIN employee AS T4 ON T1.promp_num = T3.emp_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['model_output']\n",
    "#df.iloc[0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Select(\n",
       "  expressions=[\n",
       "    Column(\n",
       "      this=Identifier(this=player, quoted=True))],\n",
       "  from=From(\n",
       "    this=Table(\n",
       "      this=Identifier(this=table_203_815, quoted=False))),\n",
       "  where=Where(\n",
       "    this=EQ(\n",
       "      this=Column(\n",
       "        this=Identifier(this=college, quoted=True)),\n",
       "      expression=Literal(this=kansas state, is_string=True))))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlglot\n",
    "from zss import simple_distance, Node\n",
    "\n",
    "def parse(output, model_output):\n",
    "    # Parse the SQL queries to ASTs\n",
    "    output_ast = sqlglot.parse(output, read='sqlite')[0]\n",
    "    model_output_ast = sqlglot.parse(model_output, read='sqlite')[0]\n",
    "    return output_ast\n",
    "\n",
    "parse(df.iloc[0]['output'], df.iloc[0]['model_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE table_17232 (\n",
      "    \"Disc\" real,\n",
      "    \"Track\" real,\n",
      "    \"English title\" text,\n",
      "    \"Japanese title\" text,\n",
      "    \"R\\u014dmaji title\" text,\n",
      "    \"Artist\" text,\n",
      "    \"Track time\" text\n",
      ")\n",
      "\n",
      "\n",
      "-- Using valid SQLite, answer the following questions for the tables provided above.\n",
      "\n",
      "-- What is the smallest track number?\n",
      "\n",
      "CREATE TABLE table_203_815 (\n",
      "    id number,\n",
      "    \"pick #\" number,\n",
      "    \"nfl team\" text,\n",
      "    \"player\" text,\n",
      "    \"position\" text,\n",
      "    \"college\" text\n",
      ")\n",
      "\n",
      "\n",
      "-- Using valid SQLite, answer the following questions for the tables provided above.\n",
      "\n",
      "-- who was the only player from kansas state ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('data/test/eng_test.parquet')\n",
    "print(df.iloc[1]['instruction'])\n",
    "print(df.iloc[0]['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/tokenizer\\\\tokenizer_config.json',\n",
       " 'models/tokenizer\\\\special_tokens_map.json',\n",
       " 'models/tokenizer\\\\spiece.model',\n",
       " 'models/tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-large')\n",
    "tokenizer.save_pretrained('models/tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
